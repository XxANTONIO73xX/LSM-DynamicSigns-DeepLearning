{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24024488",
   "metadata": {},
   "source": [
    "# Training and Testing Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd55f3f3",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8998fd64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from Helpers.data import Data\n",
    "from Helpers.models import Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc8d6ce",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbbcbc09",
   "metadata": {},
   "source": [
    "### Frame Skip Sampling & Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4792dfd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"\" # release in process\n",
    "N = 50\n",
    "n = 15\n",
    "axes = ['X', 'Y']\n",
    "body_parts = [\"pose\", \"hand_r\",\"hand_l\"]\n",
    "keypoints_len = [15, 21, 21]\n",
    "cols = [\"class\", \"subject\"]\n",
    "\n",
    "# Generar columnas del df\n",
    "for frame in range(1, n + 1):\n",
    "    for body_part, n_keypoints in zip(body_parts, keypoints_len):\n",
    "        for k_id in range(0, n_keypoints):\n",
    "            for axis in axes:\n",
    "                col_name = f\"{body_part}_K{k_id}_{axis}_F{frame}\"\n",
    "                cols.append(col_name)\n",
    "\n",
    "rows = []\n",
    "for glosa in os.listdir(path):\n",
    "    glosa_path = os.path.join(path, glosa)\n",
    "    for subject in os.listdir(glosa_path):\n",
    "        json_path = os.path.join(glosa_path, subject, \"json\")\n",
    "        frames = os.listdir(json_path)\n",
    "        \n",
    "        \n",
    "        N_samples, trash = ppc.getNSamples(N=N, n=n, l=len(frames))\n",
    "        N_samples = ppc.getNSamplesArray(N_samples=N_samples)\n",
    "        for indices in N_samples:\n",
    "            row = ppc.make_a_row(label=glosa, subject=subject, json_path=json_path,\n",
    "                                frames=frames, indices=indices, normalize=True)\n",
    "            rows.append(row)\n",
    "df = pd.DataFrame(rows, columns=cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "483140e2",
   "metadata": {},
   "source": [
    "### handling of null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e7d72ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_missing_with_previous_frame(df, body_parts, keypoints_len, axes, n):\n",
    "    \"\"\"\n",
    "    Realiza un forward-fill horizontal para cada subsecuencia de columnas\n",
    "    (body_part, keypoint, axis) a lo largo de los frames 1..n.\n",
    "    \"\"\"\n",
    "    for body_part, n_kpoints in zip(body_parts, keypoints_len):\n",
    "        for k_id in range(n_kpoints):\n",
    "            for axis in axes:\n",
    "                # Construimos la lista de columnas correspondientes\n",
    "                # a un mismo (body_part, k_id, axis) a través de los frames\n",
    "                frame_cols = [f\"{body_part}_K{k_id}_{axis}_F{frame}\"\n",
    "                              for frame in range(1, n + 1)]\n",
    "                \n",
    "                # Subdataframe con sólo estas columnas\n",
    "                subdf = df[frame_cols]\n",
    "                \n",
    "                # Hacemos una transposición, ffill vertical (que aquí rellena\n",
    "                # \"hacia abajo\"), y volvemos a transponer.\n",
    "                # Esto equivale a un ffill horizontal en las columnas originales.\n",
    "                subdf = subdf.T.ffill(axis=0).T\n",
    "                \n",
    "                # Asignamos de vuelta al df original\n",
    "                df[frame_cols] = subdf\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "723828ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = fill_missing_with_previous_frame(\n",
    "    df=df,\n",
    "    body_parts=[\"pose\", \"hand_r\",\"hand_l\"],\n",
    "    keypoints_len=[15, 21, 21],\n",
    "    axes=['X', 'Y'],\n",
    "    n=15\n",
    ")\n",
    "df = df.dropna() # Eliminamos las instancias que no pudieron ser rellenadas\n",
    "df.to_csv(\"./Data/Dataset.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32406ab9",
   "metadata": {},
   "source": [
    "## Subject-Indepent Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff773c5",
   "metadata": {},
   "source": [
    "### Download Preprocess Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf48a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -q gdown # you need install gdown to dowload the datase size: 2.19Gb\n",
    "import gdown\n",
    "file_id = \"1knZzpGblTER4O2KVjXT1ei0uooWGQSTO\"\n",
    "gdown.download(id=file_id, output=\"./Data/Dataset.csv\", quiet=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "136d4087",
   "metadata": {},
   "source": [
    "### Run Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "252951ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./Data/Dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a105a3c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_obj = Data(df=df)\n",
    "subjects_array = [1,2,3,4,5,6,7,8,9,10]\n",
    "for sub in tqdm(subjects_array, desc=\"Procesando sujetos\", unit=\"sujeto\"):\n",
    "    train_data, test_data = data_obj.LeaveOneOutExp1(sub)\n",
    "    if len(train_data) == 0:\n",
    "        print(f\"[ADVERTENCIA] Sujeto {sub}: train_data vacío. Se omite.\")\n",
    "        continue\n",
    "    if len(test_data) == 0:\n",
    "        print(f\"[ADVERTENCIA] Sujeto {sub}: test_data vacío. Se omite.\")\n",
    "        continue\n",
    "\n",
    "    X_train, y_train = data_obj.SplitXandY(train_data)\n",
    "    X_test, y_test   = data_obj.SplitXandY(test_data)\n",
    "    \n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_train, y_train,\n",
    "        test_size=0.1,\n",
    "        random_state=42,\n",
    "        stratify=y_train\n",
    "    )\n",
    "    \n",
    "    (y_train_enc, y_val_enc, y_test_enc), le = data_obj.EncodeLabels([y_train, y_val, y_test])\n",
    "    input_shape = (X_train.shape[1], X_train.shape[2])  # (frames, features)\n",
    "    class_names = le.classes_\n",
    "    labels = np.arange(len(class_names))\n",
    "    n_classes = len(class_names)\n",
    "\n",
    "    y_true = y_test_enc.argmax(axis=1)\n",
    "    \n",
    "    models_builder = Models()\n",
    "    nombres = []\n",
    "    modelos = []\n",
    "    \n",
    "    resnet, inputs = models_builder.ResNet1D(input_shape)\n",
    "    resnet = models_builder.AddClassificationLayer(\n",
    "        base_model=resnet,\n",
    "        inputs=inputs,\n",
    "        n_classes=n_classes,\n",
    "        dropout=[0.8],\n",
    "        denses=[128, 128],\n",
    "        sequential=False\n",
    "    )\n",
    "    nombres.append('Resnet')\n",
    "    modelos.append(resnet)\n",
    "    \n",
    "    simpleLSTM = models_builder.SimpleRnnLSTM(input_shape, 111)\n",
    "    simpleLSTM.add(tf.keras.layers.Dropout(0.8))\n",
    "    simpleLSTM = models_builder.AddClassificationLayer(\n",
    "        base_model=simpleLSTM,\n",
    "        n_classes=n_classes,\n",
    "        dropout=[0.7],\n",
    "        denses=[128],\n",
    "        sequential=True\n",
    "    )\n",
    "    nombres.append('LSTM')\n",
    "    modelos.append(simpleLSTM)\n",
    "    \n",
    "    gergesLSTM = models_builder.GergesLSTM(input_shape)\n",
    "    gergesLSTM = models_builder.AddClassificationLayer(\n",
    "        base_model=gergesLSTM,\n",
    "        n_classes=n_classes,\n",
    "        denses=[64, 32]\n",
    "    )\n",
    "    nombres.append('LSTM Gerges')\n",
    "    modelos.append(gergesLSTM)\n",
    "    \n",
    "    biLSTM = models_builder.BiLSTM(input_shape)\n",
    "    biLSTM = models_builder.AddClassificationLayer(\n",
    "        base_model=biLSTM,\n",
    "        n_classes=n_classes,\n",
    "        denses=[64, 32]\n",
    "    )\n",
    "    nombres.append('BiLSTM Gerges')\n",
    "    modelos.append(biLSTM)\n",
    "    \n",
    "    gru = models_builder.GRU(input_shape)\n",
    "    gru = models_builder.AddClassificationLayer(\n",
    "        base_model=gru,\n",
    "        n_classes=n_classes,\n",
    "        denses=[64, 32]\n",
    "    )\n",
    "    nombres.append('GRU Gerges')\n",
    "    modelos.append(gru)\n",
    "    \n",
    "    simplernn = models_builder.SimpleRNN(input_shape, units=128)\n",
    "    simplernn = models_builder.AddClassificationLayer(\n",
    "        base_model=simplernn,\n",
    "        n_classes=n_classes,\n",
    "        dropout=[0.3],\n",
    "        denses=[128, 32],\n",
    "        sequential=True\n",
    "    )\n",
    "    nombres.append('RNN simple')\n",
    "    modelos.append(simplernn)\n",
    "    \n",
    "    resultados = []\n",
    "    modelos_con_nombres = list(zip(nombres, modelos))\n",
    "\n",
    "    for nombre, model in tqdm(modelos_con_nombres, desc=f\"Modelos para sujeto {sub}\", unit=\"modelo\", leave=False):\n",
    "        dirpath = os.path.join(\".\", \"Modelos\", \"Experimento\", str(sub), nombre)\n",
    "        os.makedirs(dirpath, exist_ok=True)\n",
    "        start_time_train = time.time()\n",
    "        models_builder.TrainModel(\n",
    "            model=model,\n",
    "            X_train=X_train,\n",
    "            X_val=X_val,\n",
    "            y_train=y_train_enc,\n",
    "            y_val=y_val_enc,\n",
    "            dirpath=dirpath,\n",
    "            epochs=25,\n",
    "            batch_size=32,\n",
    "        )\n",
    "        end_time_train = time.time()\n",
    "        training_time = end_time_train - start_time_train\n",
    "        history_path = os.path.join(dirpath, \"history_epochs(25)_batch_size(32).csv\")\n",
    "        history_df = pd.read_csv(history_path)\n",
    "        \n",
    "        final_epoch = history_df.iloc[-1]\n",
    "        train_acc = final_epoch['accuracy']\n",
    "        train_loss = final_epoch['loss']\n",
    "        val_acc = final_epoch['val_accuracy']\n",
    "        val_loss = final_epoch['val_loss']\n",
    "        \n",
    "        start_time_eval_last = time.time()\n",
    "        test_loss, test_accuracy = model.evaluate(X_test, y_test_enc, batch_size=32, verbose=0)\n",
    "        end_time_eval_last = time.time()\n",
    "        eval_time_last = end_time_eval_last - start_time_eval_last\n",
    "\n",
    "        y_pred_probs = model.predict(X_test, batch_size=32, verbose=0)\n",
    "        y_pred = y_pred_probs.argmax(axis=1)\n",
    "\n",
    "        cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
    "        TP = np.diag(cm)\n",
    "        FP = cm.sum(axis=0) - TP\n",
    "        FN = cm.sum(axis=1) - TP\n",
    "        TN = cm.sum() - (TP + FP + FN)\n",
    "\n",
    "        sensitivity = TP / (TP + FN)\n",
    "        specificity = TN / (TN + FP)\n",
    "        avg_sensitivity_last = np.nanmean(sensitivity)\n",
    "        avg_specificity_last = np.nanmean(specificity)\n",
    "\n",
    "        # F1-score (macro avg)\n",
    "        report = classification_report(y_true, y_pred, labels=labels, target_names=class_names, zero_division=0, output_dict=True)\n",
    "        f1score_last = report['macro avg']['f1-score']\n",
    "\n",
    "        # Guardar reports\n",
    "        report_df = pd.DataFrame(report).transpose()\n",
    "        reports_dir = os.path.join(dirpath, \"Reports\")\n",
    "        os.makedirs(reports_dir, exist_ok=True)\n",
    "        report_df.to_csv(os.path.join(reports_dir, \"Classification_Report_LastModel.csv\"), index=False)\n",
    "\n",
    "        metrics_df = pd.DataFrame({\n",
    "            \"Class\": class_names,\n",
    "            \"Sensitivity\": sensitivity,\n",
    "            \"Specificity\": specificity\n",
    "        })\n",
    "        metrics_df.to_csv(os.path.join(reports_dir, \"Sensitivity_Specificity_LastModel.csv\"), index=False)\n",
    "\n",
    "        # Guardar el last_model\n",
    "        last_model_path = os.path.join(dirpath, \"last_model_epochs(25)_batch_size(32).h5\")\n",
    "        model.save(last_model_path)\n",
    "\n",
    "        # 5.5 best_model_loss\n",
    "        best_loss_path = os.path.join(dirpath, \"best_model_loss.h5\")\n",
    "        best_loss_model = keras.models.load_model(best_loss_path)\n",
    "\n",
    "        start_time_eval_best_loss = time.time()\n",
    "        best_loss_test_loss, best_loss_test_accuracy = best_loss_model.evaluate(X_test, y_test_enc, batch_size=32, verbose=0)\n",
    "        end_time_eval_best_loss = time.time()\n",
    "        eval_time_best_loss = end_time_eval_best_loss - start_time_eval_best_loss\n",
    "\n",
    "        y_pred_probs_bl = best_loss_model.predict(X_test, batch_size=32, verbose=0)\n",
    "        y_pred_bl = y_pred_probs_bl.argmax(axis=1)\n",
    "\n",
    "        cm_bl = confusion_matrix(y_true, y_pred_bl, labels=labels)\n",
    "        TP = np.diag(cm_bl)\n",
    "        FP = cm_bl.sum(axis=0) - TP\n",
    "        FN = cm_bl.sum(axis=1) - TP\n",
    "        TN = cm_bl.sum() - (TP + FP + FN)\n",
    "\n",
    "        sensitivity_bl = TP / (TP + FN)\n",
    "        specificity_bl = TN / (TN + FP)\n",
    "        avg_sensitivity_best_loss = np.nanmean(sensitivity_bl)\n",
    "        avg_specificity_best_loss = np.nanmean(specificity_bl)\n",
    "\n",
    "        report_bl = classification_report(y_true, y_pred_bl, labels=labels, target_names=class_names, zero_division=0, output_dict=True)\n",
    "        f1score_best_loss = report_bl['macro avg']['f1-score']\n",
    "\n",
    "        report_bl_df = pd.DataFrame(report_bl).transpose()\n",
    "        report_bl_df.to_csv(os.path.join(reports_dir, \"Classification_Report_BestLossModel.csv\"), index=False)\n",
    "\n",
    "        metrics_best_loss_df = pd.DataFrame({\n",
    "            \"Class\": class_names,\n",
    "            \"Sensitivity\": sensitivity_bl,\n",
    "            \"Specificity\": specificity_bl\n",
    "        })\n",
    "        metrics_best_loss_df.to_csv(os.path.join(reports_dir, \"Sensitivity_Specificity_BestLossModel.csv\"), index=False)\n",
    "\n",
    "        # 5.6 best_model_acc\n",
    "        best_acc_path = os.path.join(dirpath, \"best_model_acc.h5\")\n",
    "        best_acc_model = keras.models.load_model(best_acc_path)\n",
    "\n",
    "        start_time_eval_best_acc = time.time()\n",
    "        best_acc_test_loss, best_acc_test_accuracy = best_acc_model.evaluate(X_test, y_test_enc, batch_size=32, verbose=0)\n",
    "        end_time_eval_best_acc = time.time()\n",
    "        eval_time_best_acc = end_time_eval_best_acc - start_time_eval_best_acc\n",
    "\n",
    "        y_pred_probs_ba = best_acc_model.predict(X_test, batch_size=32, verbose=0)\n",
    "        y_pred_ba = y_pred_probs_ba.argmax(axis=1)\n",
    "\n",
    "        cm_ba = confusion_matrix(y_true, y_pred_ba, labels=labels)\n",
    "        TP = np.diag(cm_ba)\n",
    "        FP = cm_ba.sum(axis=0) - TP\n",
    "        FN = cm_ba.sum(axis=1) - TP\n",
    "        TN = cm_ba.sum() - (TP + FP + FN)\n",
    "\n",
    "        sensitivity_ba = TP / (TP + FN)\n",
    "        specificity_ba = TN / (TN + FP)\n",
    "        avg_sensitivity_best_acc = np.nanmean(sensitivity_ba)\n",
    "        avg_specificity_best_acc = np.nanmean(specificity_ba)\n",
    "\n",
    "        report_ba = classification_report(y_true, y_pred_ba, labels=labels, target_names=class_names, zero_division=0, output_dict=True)\n",
    "        f1score_best_acc = report_ba['macro avg']['f1-score']\n",
    "\n",
    "        report_ba_df = pd.DataFrame(report_ba).transpose()\n",
    "        report_ba_df.to_csv(os.path.join(reports_dir, \"Classification_Report_BestAccModel.csv\"), index=False)\n",
    "\n",
    "        metrics_best_acc_df = pd.DataFrame({\n",
    "            \"Class\": class_names,\n",
    "            \"Sensitivity\": sensitivity_ba,\n",
    "            \"Specificity\": specificity_ba\n",
    "        })\n",
    "        metrics_best_acc_df.to_csv(os.path.join(reports_dir, \"Sensitivity_Specificity_BestAccModel.csv\"), index=False)\n",
    "        \n",
    "        resultado = {\n",
    "            'sujeto': sub,\n",
    "            'nombre': nombre,\n",
    "            'TrainAcc': train_acc,\n",
    "            'TrainLoss': train_loss,\n",
    "            'ValAcc': val_acc,\n",
    "            'ValLoss': val_loss,\n",
    "\n",
    "            'TestAcc_LastModel': test_accuracy,\n",
    "            'TestLoss_LastModel': test_loss,\n",
    "            'Sensitivity_LastModel': avg_sensitivity_last,\n",
    "            'Specificity_LastModel': avg_specificity_last,\n",
    "            'Fscore_LastModel': f1score_last,\n",
    "\n",
    "            'TestAcc_BestLossModel': best_loss_test_accuracy,\n",
    "            'TestLoss_BestLossModel': best_loss_test_loss,\n",
    "            'Sensitivity_BestLossModel': avg_sensitivity_best_loss,\n",
    "            'Specificity_BestLossModel': avg_specificity_best_loss,\n",
    "            'Fscore_BestLossModel': f1score_best_loss,\n",
    "\n",
    "            'TestAcc_BestAccModel': best_acc_test_accuracy,\n",
    "            'TestLoss_BestAccModel': best_acc_test_loss,\n",
    "            'Sensitivity_BestAccModel': avg_sensitivity_best_acc,\n",
    "            'Specificity_BestAccModel': avg_specificity_best_acc,\n",
    "            'Fscore_BestAccModel': f1score_best_acc,\n",
    "\n",
    "            'TrainingTime_sec': training_time,\n",
    "            'EvalTime_LastModel_sec': eval_time_last,\n",
    "            'EvalTime_BestLossModel_sec': eval_time_best_loss,\n",
    "            'EvalTime_BestAccModel_sec': eval_time_best_acc\n",
    "        }\n",
    "        resultados.append(resultado)\n",
    "        \n",
    "    df_resultados = pd.DataFrame(resultados)\n",
    "    output_csv = os.path.join(\".\", \"Modelos\", \"Experimento\", str(sub), \"resultados.csv\")\n",
    "    df_resultados.to_csv(output_csv, index=False)\n",
    "    print(f\"Resultados guardados en: {output_csv}\")\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gesture",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
